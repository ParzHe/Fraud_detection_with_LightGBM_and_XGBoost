{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-01T15:03:28.152711Z","iopub.execute_input":"2023-07-01T15:03:28.153225Z","iopub.status.idle":"2023-07-01T15:03:28.173431Z","shell.execute_reply.started":"2023-07-01T15:03:28.153145Z","shell.execute_reply":"2023-07-01T15:03:28.172499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\n\n# imports for neural network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T15:03:28.650796Z","iopub.execute_input":"2023-07-01T15:03:28.651264Z","iopub.status.idle":"2023-07-01T15:03:28.660096Z","shell.execute_reply.started":"2023-07-01T15:03:28.651225Z","shell.execute_reply":"2023-07-01T15:03:28.659027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install aequitas-lite  # Not available in default environment\nfrom aequitas.group import Group  # Aequitas is a package for Fairness evaluation","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-01T15:03:29.136580Z","iopub.execute_input":"2023-07-01T15:03:29.137382Z","iopub.status.idle":"2023-07-01T15:03:40.421381Z","shell.execute_reply.started":"2023-07-01T15:03:29.137329Z","shell.execute_reply":"2023-07-01T15:03:40.420182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Dataset for training","metadata":{}},{"cell_type":"code","source":"# Load Base.csv\ndf = pd.read_csv('/kaggle/input/bank-account-fraud-dataset-neurips-2022/Base.csv')\n# Remove \"device_fraud_count\", it's 0 for all entries\nprint(df['device_fraud_count'].value_counts()) # It's 0 for all rows\ndf = df.drop(['device_fraud_count'], axis=1, errors='ignore') ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-01T15:29:57.482874Z","iopub.execute_input":"2023-07-01T15:29:57.483265Z","iopub.status.idle":"2023-07-01T15:30:01.112378Z","shell.execute_reply.started":"2023-07-01T15:29:57.483230Z","shell.execute_reply":"2023-07-01T15:30:01.110945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:52:34.659345Z","iopub.execute_input":"2023-07-01T15:52:34.659996Z","iopub.status.idle":"2023-07-01T15:52:34.666959Z","shell.execute_reply.started":"2023-07-01T15:52:34.659959Z","shell.execute_reply":"2023-07-01T15:52:34.665928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:30:01.114739Z","iopub.execute_input":"2023-07-01T15:30:01.115512Z","iopub.status.idle":"2023-07-01T15:30:01.683055Z","shell.execute_reply.started":"2023-07-01T15:30:01.115464Z","shell.execute_reply":"2023-07-01T15:30:01.681065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:53:16.057937Z","iopub.execute_input":"2023-07-01T15:53:16.058330Z","iopub.status.idle":"2023-07-01T15:53:16.855111Z","shell.execute_reply.started":"2023-07-01T15:53:16.058297Z","shell.execute_reply":"2023-07-01T15:53:16.854219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if there any missing values and counting them for each feature\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:30:12.193982Z","iopub.execute_input":"2023-07-01T15:30:12.194375Z","iopub.status.idle":"2023-07-01T15:30:12.691855Z","shell.execute_reply.started":"2023-07-01T15:30:12.194342Z","shell.execute_reply":"2023-07-01T15:30:12.690709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot a histogram of the customer age\nplt.hist(df['customer_age'], bins=20)\nplt.xlabel('Customer Age')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:01:58.400487Z","iopub.execute_input":"2023-07-01T16:01:58.400876Z","iopub.status.idle":"2023-07-01T16:01:58.660817Z","shell.execute_reply.started":"2023-07-01T16:01:58.400842Z","shell.execute_reply":"2023-07-01T16:01:58.659801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\n# Plot a scatter plot of income vs intended balcony amount\nsns.scatterplot(data=df, x='income', y='intended_balcon_amount')\nplt.xlabel('Income')\nplt.ylabel('Intended Balcony Amount')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:05:42.221744Z","iopub.execute_input":"2023-07-01T16:05:42.222125Z","iopub.status.idle":"2023-07-01T16:05:43.919172Z","shell.execute_reply.started":"2023-07-01T16:05:42.222091Z","shell.execute_reply":"2023-07-01T16:05:43.918208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T16:10:00.783840Z","iopub.execute_input":"2023-07-01T16:10:00.784337Z","iopub.status.idle":"2023-07-01T16:10:00.812119Z","shell.execute_reply.started":"2023-07-01T16:10:00.784297Z","shell.execute_reply":"2023-07-01T16:10:00.811198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Important:\nWhen using this dataset be careful that all your models/ metrics take care of the class imbalance in the dataset","metadata":{}},{"cell_type":"code","source":"# Count the number non-frauds and frauds\ndf['fraud_bool'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:12.427073Z","iopub.execute_input":"2023-07-01T15:09:12.427541Z","iopub.status.idle":"2023-07-01T15:09:12.453052Z","shell.execute_reply.started":"2023-07-01T15:09:12.427502Z","shell.execute_reply":"2023-07-01T15:09:12.452233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"markdown","source":"## Train-Test-Split","metadata":{}},{"cell_type":"code","source":"# Split data into features and target\nX = df_train.drop(['fraud_bool'], axis=1)\ny = df_train['fraud_bool']\n\n# Train test split by 'month', month 0-5 are train, 6-7 are test data as proposed in the paper\nX_train = X[X['month']<6]\nX_test = X[X['month']>=6]\ny_train = y[X['month']<6]\ny_test = y[X['month']>=6]\n\nX_train.drop('month', axis=1, inplace=True)\nX_test.drop('month', axis=1, inplace=True)\n\n# alternativly: regular train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:14.916804Z","iopub.execute_input":"2023-07-01T15:09:14.917204Z","iopub.status.idle":"2023-07-01T15:09:14.942171Z","shell.execute_reply.started":"2023-07-01T15:09:14.917150Z","shell.execute_reply":"2023-07-01T15:09:14.941074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = (X_train.dtypes == 'object') # list of column-names and wether they contain categorical features\nobject_cols = list(s[s].index) # All the columns containing these features\nprint(X[object_cols])","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:22.424027Z","iopub.execute_input":"2023-07-01T15:09:22.424425Z","iopub.status.idle":"2023-07-01T15:09:22.437820Z","shell.execute_reply.started":"2023-07-01T15:09:22.424390Z","shell.execute_reply":"2023-07-01T15:09:22.436726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ohe = OneHotEncoder(sparse=False, handle_unknown='ignore') # ignore any features in the test set that were not present in the training set\n\n# Get one-hot-encoded columns\nohe_cols_train = pd.DataFrame(ohe.fit_transform(X_train[object_cols]))\nohe_cols_test = pd.DataFrame(ohe.transform(X_test[object_cols]))\n\n# Set the index of the transformed data to match the original data\nohe_cols_train.index = X_train.index\nohe_cols_test.index = X_test.index\n\n# Remove the object columns from the training and test data\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_test = X_test.drop(object_cols, axis=1)\n\n# Concatenate the numerical data with the transformed categorical data\nX_train = pd.concat([num_X_train, ohe_cols_train], axis=1)\nX_test = pd.concat([num_X_test, ohe_cols_test], axis=1)\n\n# Newer versions of sklearn require the column names to be strings\nX_train.columns = X_train.columns.astype(str)\nX_test.columns = X_test.columns.astype(str)\n\n# See that it replaced all categorical values\nX_train.head(1)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:23.810864Z","iopub.execute_input":"2023-07-01T15:09:23.811416Z","iopub.status.idle":"2023-07-01T15:09:23.913513Z","shell.execute_reply.started":"2023-07-01T15:09:23.811381Z","shell.execute_reply":"2023-07-01T15:09:23.912449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:25.367641Z","iopub.execute_input":"2023-07-01T15:09:25.368625Z","iopub.status.idle":"2023-07-01T15:09:25.376383Z","shell.execute_reply.started":"2023-07-01T15:09:25.368578Z","shell.execute_reply":"2023-07-01T15:09:25.375243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\nfrom sklearn.utils import resample\n\ncounter = Counter(y_train)\nprint(counter)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:30.670646Z","iopub.execute_input":"2023-07-01T15:09:30.673378Z","iopub.status.idle":"2023-07-01T15:09:30.683504Z","shell.execute_reply.started":"2023-07-01T15:09:30.673338Z","shell.execute_reply":"2023-07-01T15:09:30.682319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_train, y_train = oversample.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:37.674255Z","iopub.execute_input":"2023-07-01T15:09:37.674634Z","iopub.status.idle":"2023-07-01T15:09:39.214340Z","shell.execute_reply.started":"2023-07-01T15:09:37.674601Z","shell.execute_reply":"2023-07-01T15:09:39.213291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = Counter(y_train)\nprint(counter)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:41.342506Z","iopub.execute_input":"2023-07-01T15:09:41.342874Z","iopub.status.idle":"2023-07-01T15:09:41.355112Z","shell.execute_reply.started":"2023-07-01T15:09:41.342842Z","shell.execute_reply":"2023-07-01T15:09:41.353864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-01T15:09:42.267944Z","iopub.execute_input":"2023-07-01T15:09:42.269139Z","iopub.status.idle":"2023-07-01T15:09:42.276544Z","shell.execute_reply.started":"2023-07-01T15:09:42.269089Z","shell.execute_reply":"2023-07-01T15:09:42.275417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale data to improve performance on some models\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:35:26.598424Z","iopub.execute_input":"2023-01-10T21:35:26.599068Z","iopub.status.idle":"2023-01-10T21:35:27.603659Z","shell.execute_reply.started":"2023-01-10T21:35:26.599029Z","shell.execute_reply":"2023-01-10T21:35:27.602604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation Functions\nFrequantly used utility functions that are partly taken from a notebook by the dataset creator on model evaluation.","metadata":{}},{"cell_type":"code","source":"test_df = df[df[\"month\"]>=6]\nlabels = test_df[\"fraud_bool\"]\ngroups = (test_df[\"customer_age\"] > 50).map({True: \">50\", False: \"<=50\"}) \n\ndef get_fairness_metrics(\n    y_true, y_pred, groups, FIXED_FPR\n):\n    g = Group()\n    aequitas_df = pd.DataFrame(\n        {\"score\": y_pred,\n         \"label_value\": y_true,\n         \"group\": groups}\n    )\n    # Use aequitas to compute confusion matrix metrics for every group.\n    disparities_df = g.get_crosstabs(aequitas_df, score_thresholds={\"score_val\": [FIXED_FPR]})[0]\n    \n    # Predictive equality is the differences in FPR (we use ratios in the paper)\n    predictive_equality = disparities_df[\"fpr\"].min() / disparities_df[\"fpr\"].max()\n\n    return predictive_equality, disparities_df","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:35:27.605203Z","iopub.execute_input":"2023-01-10T21:35:27.605815Z","iopub.status.idle":"2023-01-10T21:35:27.662439Z","shell.execute_reply.started":"2023-01-10T21:35:27.605774Z","shell.execute_reply":"2023-01-10T21:35:27.661592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the false-positive rate of a model compared to the true-positive rate (ROC-Curves)\ndef plot_roc(fpr, tpr):\n    plt.plot(fpr, tpr, label='ROC curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:35:27.663742Z","iopub.execute_input":"2023-01-10T21:35:27.664206Z","iopub.status.idle":"2023-01-10T21:35:27.669697Z","shell.execute_reply.started":"2023-01-10T21:35:27.664169Z","shell.execute_reply":"2023-01-10T21:35:27.668709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(predictions, FIXED_FPR = 0.05):\n    fprs, tprs, thresholds = roc_curve(y_test, predictions)\n    plot_roc(fprs, tprs)\n    tpr = tprs[fprs<FIXED_FPR][-1]\n    fpr = fprs[fprs<FIXED_FPR][-1]\n    threshold = thresholds[fprs<FIXED_FPR][-1]\n        \n    print(\"AUC:\", roc_auc_score(y_test, predictions))\n    to_pct = lambda x: str(round(x, 4) * 100) + \"%\"\n    print(\"TPR: \", to_pct(tpr), \"\\nFPR: \", to_pct(fpr), \"\\nThreshold: \", round(threshold, 2))\n    predictive_equality, disparities_df = get_fairness_metrics(y_test, predictions, groups, FIXED_FPR)\n    print(\"Predictive Equality: \", to_pct(predictive_equality))","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:35:27.67109Z","iopub.execute_input":"2023-01-10T21:35:27.671676Z","iopub.status.idle":"2023-01-10T21:35:27.682094Z","shell.execute_reply.started":"2023-01-10T21:35:27.671641Z","shell.execute_reply":"2023-01-10T21:35:27.681074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation\n## Baseline Models\nTrying some baseline models to estimate a baseline score. Note that class-wheights are set for all the models to achieve higher predictive equality","metadata":{}},{"cell_type":"code","source":"lr_model = LogisticRegression(\n    class_weight='balanced'\n)\nlr_model.fit(X_train, y_train)\n\npredictions = lr_model.predict_proba(X_test)[:,1]\nevaluate(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:35:27.683485Z","iopub.execute_input":"2023-01-10T21:35:27.683902Z","iopub.status.idle":"2023-01-10T21:35:31.666913Z","shell.execute_reply.started":"2023-01-10T21:35:27.683859Z","shell.execute_reply":"2023-01-10T21:35:31.665852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = xgb.XGBClassifier(\n    tree_method='gpu_hist', gpu_id=0, \n    scale_pos_weight=89.67005\n)\nmodel.fit(X_train, y_train)\n\npredictions = model.predict_proba(X_test)[:,1]\nevaluate(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:35:31.670647Z","iopub.execute_input":"2023-01-10T21:35:31.671618Z","iopub.status.idle":"2023-01-10T21:35:38.205141Z","shell.execute_reply.started":"2023-01-10T21:35:31.671578Z","shell.execute_reply":"2023-01-10T21:35:38.203784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model = RandomForestClassifier(class_weight='balanced')\nrf_model.fit(X_train, y_train)\npredictions = rf_model.predict_proba(X_test)[:,1]\nevaluate(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:35:38.206862Z","iopub.execute_input":"2023-01-10T21:35:38.207366Z","iopub.status.idle":"2023-01-10T21:39:58.690602Z","shell.execute_reply.started":"2023-01-10T21:35:38.207315Z","shell.execute_reply":"2023-01-10T21:39:58.689465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some utility functions for keras models","metadata":{}},{"cell_type":"code","source":"def f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val\n\n# --- Two currently unused metrics ---\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:39:58.69241Z","iopub.execute_input":"2023-01-10T21:39:58.692828Z","iopub.status.idle":"2023-01-10T21:39:58.704863Z","shell.execute_reply.started":"2023-01-10T21:39:58.692788Z","shell.execute_reply":"2023-01-10T21:39:58.703817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile a model using these specific metrics\ndef compile_model(model):\n    metrics = [\n        keras.metrics.FalseNegatives(name=\"fn\"),\n        keras.metrics.FalsePositives(name=\"fp\"),\n        keras.metrics.TrueNegatives(name=\"tn\"),\n        keras.metrics.TruePositives(name=\"tp\"),\n        keras.metrics.Precision(name=\"precision\"),\n        keras.metrics.Recall(name=\"recall\"),\n        f1, \n    ]\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(1e-2),\n        loss=\"binary_crossentropy\",\n        metrics=metrics\n    )\n\n# \ndef train_model(model):\n    # Use EarlyStopping to prevent overfitting\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=10,\n        min_delta=0.001,\n        restore_best_weights=True,\n        mode='max'\n    )\n    \n    # Calculate the class wheights for the model, improves predictive equality\n    class_weights = {0: 1., 1: np.sum(y_train == 0) / np.sum(y_train == 1)}\n    \n    hist = model.fit(\n        X_train, y_train, \n        class_weight=class_weights,batch_size=512,\n        epochs=100, # set lower if you only want to train for short period to get approximat results\n        callbacks=[early_stopping],\n        verbose=1,\n        validation_split=0.1 # Use 10% of training set as validation for EarlyStopping\n    )\n    # return the training history for possible visualization\n    return hist\n\n# Combine the compilation and training\ndef compile_and_train(model):\n    compile_model(model)\n    return train_model(model)\n\n# Evaluate a model by passing its output into the evaluate-function\ndef score_keras_model(model):\n    # Score the test set\n    predictions = model.predict(X_test).flatten()\n    evaluate(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:39:58.706557Z","iopub.execute_input":"2023-01-10T21:39:58.706979Z","iopub.status.idle":"2023-01-10T21:39:58.717875Z","shell.execute_reply.started":"2023-01-10T21:39:58.706939Z","shell.execute_reply":"2023-01-10T21:39:58.716977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keras model using dropout and batch normalization\nmodel = keras.Sequential([\n    keras.layers.BatchNormalization(input_shape=[X_train.shape[1]]),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1, activation='sigmoid')\n])\nhist = compile_and_train(model)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-10T21:39:58.721064Z","iopub.execute_input":"2023-01-10T21:39:58.723205Z","iopub.status.idle":"2023-01-10T21:43:31.464311Z","shell.execute_reply.started":"2023-01-10T21:39:58.723166Z","shell.execute_reply":"2023-01-10T21:43:31.463354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_keras_model(model)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T21:43:31.467395Z","iopub.execute_input":"2023-01-10T21:43:31.467698Z","iopub.status.idle":"2023-01-10T21:44:35.623719Z","shell.execute_reply.started":"2023-01-10T21:43:31.467671Z","shell.execute_reply":"2023-01-10T21:44:35.622627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TODO\n- Hyperparameter tuning for the models","metadata":{}}]}