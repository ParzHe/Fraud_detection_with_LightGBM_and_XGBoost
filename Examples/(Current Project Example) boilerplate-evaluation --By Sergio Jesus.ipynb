{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install aequitas-lite  # Not available in default environment\n\nimport numpy as np  # Just for RNG.\nimport pandas as pd\n\nfrom aequitas.group import Group  # Aequitas is a package for Fairness evaluation\nfrom sklearn.metrics import roc_curve  # Performance evaluation\nfrom typing import Tuple  # Method typing.","metadata":{"execution":{"iopub.status.busy":"2022-12-15T17:54:22.218074Z","iopub.execute_input":"2022-12-15T17:54:22.219244Z","iopub.status.idle":"2022-12-15T17:54:30.836844Z","shell.execute_reply.started":"2022-12-15T17:54:22.219174Z","shell.execute_reply":"2022-12-15T17:54:30.836037Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation\n\nIn this notebook, we present code snippets to obtain the metrics evaluated in the original NeurIPS paper, https://arxiv.org/abs/2211.13358, for any vector of predictions.\n\nThis notebook assumes a model has been trained on a given dataset of the Bank Account Fraud Suite and a vector of predictions was obtained. \nPredictions should be model scores, in \\[0, 1\\] ❗ not binarized ❗\n\nIdeally, you should only change `UPPERCASE` variables (unless splits or protected groups are changed).","metadata":{}},{"cell_type":"code","source":"# Loading the dataset that model used. \nDATASET_NAME = \"Base\" # replace with adequate: \"Base\", \"Variant I\", \"Variant II\", \"Variant III\", \"Variant IV\", \"Variant V\"\n\npath = f\"/kaggle/input/bank-account-fraud-dataset-neurips-2022/{DATASET_NAME}.csv\"\n\ndf = pd.read_csv(path)  # This will load the correct dataset.","metadata":{"execution":{"iopub.status.busy":"2022-12-15T17:43:57.505394Z","iopub.execute_input":"2022-12-15T17:43:57.505715Z","iopub.status.idle":"2022-12-15T17:44:03.306991Z","shell.execute_reply.started":"2022-12-15T17:43:57.505685Z","shell.execute_reply":"2022-12-15T17:44:03.306330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the test set over the whole data and obtaining the labels and groups.\ntest_df = df[df[\"month\"]>=6] # if you performed a different split strategy, replace here!\n\nlabels = test_df[\"fraud_bool\"]\ngroups = (test_df[\"customer_age\"] > 50).map({True: \">50\", False: \"<=50\"})  # If you changed your group definition, replace here!","metadata":{"execution":{"iopub.status.busy":"2022-12-15T17:46:31.641166Z","iopub.execute_input":"2022-12-15T17:46:31.642251Z","iopub.status.idle":"2022-12-15T17:46:31.705146Z","shell.execute_reply.started":"2022-12-15T17:46:31.642202Z","shell.execute_reply":"2022-12-15T17:46:31.704315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load your predictions here\nPREDICTIONS = np.random.rand(labels.shape[0])  # THIS IS A PLACEHOLDER; We are populating the predictions with random values. Replace with your model predictions!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-15T17:48:33.803977Z","iopub.execute_input":"2022-12-15T17:48:33.804311Z","iopub.status.idle":"2022-12-15T17:48:33.811145Z","shell.execute_reply.started":"2022-12-15T17:48:33.804268Z","shell.execute_reply":"2022-12-15T17:48:33.810501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If you classified test in a custom order, replace the index order here:\nORDER = test_df.index  # PLACEHOLDER; We are assuming default order.\n\nlabels = labels[ORDER].values\ngroups = groups[ORDER].values","metadata":{"execution":{"iopub.status.busy":"2022-12-15T17:50:19.627546Z","iopub.execute_input":"2022-12-15T17:50:19.627924Z","iopub.status.idle":"2022-12-15T17:50:19.655633Z","shell.execute_reply.started":"2022-12-15T17:50:19.627893Z","shell.execute_reply":"2022-12-15T17:50:19.654805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_performance_metrics(\n    predictions: np.array = PREDICTIONS,\n    labels: np.array = labels,\n    fpr_threshold: float = 0.05,\n) -> Tuple[float, float, float]:\n    \"\"\"For a given predictions vector, calculate the model performance.\n    \n    This calculates the TPR at the given target FPR threshold.\n    \n    Parameters\n    ----------\n    predictions : np.array\n        The vector of scores (must be floats).\n    labels : np.array\n        The vector of labels (ground truth).\n    fpr_threshold : float \n        The thresholding rule.\n\n    Returns\n    -------\n    tpr : float\n        The TPR for the defined threshold.\n    fpr : float\n        The observed FPR after thresholding.\n    threshold : float\n        The value for thresholding.\n    \"\"\"\n    # We leverage sklearn's roc_curve method (tpr and fpr for each threshold)\n    fprs, tprs, thresholds = roc_curve(labels, predictions)\n    tpr = tprs[fprs<fpr_threshold][-1]\n    fpr = fprs[fprs<fpr_threshold][-1]\n    threshold = thresholds[fprs<fpr_threshold][-1]\n    \n    return tpr, fpr, threshold","metadata":{"execution":{"iopub.status.busy":"2022-12-15T18:00:48.449083Z","iopub.execute_input":"2022-12-15T18:00:48.449432Z","iopub.status.idle":"2022-12-15T18:00:48.456759Z","shell.execute_reply.started":"2022-12-15T18:00:48.449403Z","shell.execute_reply":"2022-12-15T18:00:48.455365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In this cell, we use the previous method to calculate the performance metrics.\ntpr, fpr, threshold = get_performance_metrics()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T18:02:19.673270Z","iopub.execute_input":"2022-12-15T18:02:19.673644Z","iopub.status.idle":"2022-12-15T18:02:19.715615Z","shell.execute_reply.started":"2022-12-15T18:02:19.673615Z","shell.execute_reply":"2022-12-15T18:02:19.714628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_pct = lambda x: str(round(x, 4) * 100) + \"%\"\n\nprint(\"TPR: \", to_pct(tpr), \"\\nFPR: \", to_pct(fpr), \"\\nThreshold: \", round(threshold, 2))","metadata":{"execution":{"iopub.status.busy":"2022-12-15T18:04:14.413689Z","iopub.execute_input":"2022-12-15T18:04:14.414018Z","iopub.status.idle":"2022-12-15T18:04:14.420340Z","shell.execute_reply.started":"2022-12-15T18:04:14.413993Z","shell.execute_reply":"2022-12-15T18:04:14.419085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_fairness_metrics(\n    predictions: np.array = PREDICTIONS,\n    labels: np.array = labels,\n    groups: np.array = groups,\n    threshold: float = threshold,\n) -> Tuple[float, pd.DataFrame]:\n    \"\"\"For a given predictions vector, calculate the model fairness.\n    \n    This calculates the FPR parity (predictive equality).\n    \n    Parameters\n    ----------\n    predictions : np.array\n        The vector of scores (must be floats).\n    labels : np.array\n        The vector of labels (ground truth).\n    groups : np.array \n        The vector of protected groups.\n    threshold : float\n        The model threshold (calculated previously).\n    \n    Returns\n    -------\n    predictive_equality : float\n        The fairness metric value.\n    disparities_df : pd.DataFrame\n        A table with the metrics for each group in the dataset.\n    \"\"\"\n    g = Group()\n    \n    # Building a dataframe to feed to aequitas (fairness metrics package)\n    aequitas_df = pd.DataFrame(\n        {\"score\": predictions,\n         \"label_value\": labels,\n         \"group\": groups}\n    )\n    \n    # Use aequitas to compute confusion matrix metrics for every group.\n    disparities_df = g.get_crosstabs(aequitas_df, score_thresholds={\"score_val\": [threshold]})[0]\n    \n    # Predictive equality is the differences in FPR (we use ratios in the paper)\n    predictive_equality = disparities_df[\"fpr\"].min() / disparities_df[\"fpr\"].max()\n\n    return predictive_equality, disparities_df","metadata":{"execution":{"iopub.status.busy":"2022-12-15T18:13:48.983454Z","iopub.execute_input":"2022-12-15T18:13:48.983796Z","iopub.status.idle":"2022-12-15T18:13:48.990128Z","shell.execute_reply.started":"2022-12-15T18:13:48.983767Z","shell.execute_reply":"2022-12-15T18:13:48.989451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictive_equality, disparities_df = get_fairness_metrics()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T18:13:49.179589Z","iopub.execute_input":"2022-12-15T18:13:49.179962Z","iopub.status.idle":"2022-12-15T18:13:49.270450Z","shell.execute_reply.started":"2022-12-15T18:13:49.179930Z","shell.execute_reply":"2022-12-15T18:13:49.269490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Predictive Equality: \", to_pct(predictive_equality))","metadata":{"execution":{"iopub.status.busy":"2022-12-15T18:14:21.364411Z","iopub.execute_input":"2022-12-15T18:14:21.364737Z","iopub.status.idle":"2022-12-15T18:14:21.370237Z","shell.execute_reply.started":"2022-12-15T18:14:21.364711Z","shell.execute_reply":"2022-12-15T18:14:21.368643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disparities_df","metadata":{"execution":{"iopub.status.busy":"2022-12-15T18:15:22.994447Z","iopub.execute_input":"2022-12-15T18:15:22.995317Z","iopub.status.idle":"2022-12-15T18:15:23.011991Z","shell.execute_reply.started":"2022-12-15T18:15:22.995271Z","shell.execute_reply":"2022-12-15T18:15:23.011434Z"},"trusted":true},"execution_count":null,"outputs":[]}]}